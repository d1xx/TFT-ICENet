{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8f0d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape (4111, 5)\n",
      "test_x.shape (4000, 96, 5)\n",
      "test_y.shape (4000, 16, 1)\n",
      "(4000, 96, 5) (4000, 16, 1)\n",
      "allnumber 4000\n",
      "trainnum 3200\n",
      "testnum 800\n",
      "\n",
      "torch.Size([128, 96, 5])\n",
      "torch.Size([128, 16, 1])\n",
      "----------------\n",
      "torch.Size([128, 96, 5])\n",
      "torch.Size([128, 16, 1])\n",
      "y_true (4015, 1)\n",
      "y_true_train (3215, 1)\n",
      "y_true_test (815, 1)\n"
     ]
    }
   ],
   "source": [
    "%run \"建立深度学习数据集 tensor.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d9fad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24eb1ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, feature_x=5, feature_y=1, hidden=64, num_layers=3, time_window=96, step=16):#hidden一般2/4/16的倍数\n",
    "        super(ANN, self).__init__()\n",
    "        self.feature_y = feature_y\n",
    "        self.step = step\n",
    "        \n",
    "        flattened_input_dim = feature_x * time_window\n",
    "        \n",
    "        self.fc1 = nn.Linear(flattened_input_dim, hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "         \n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.elu3 = nn.ELU()\n",
    "\n",
    "        self.fc_time = nn.Linear(hidden, step * feature_y) # 64，16\n",
    "     \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "         \n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh2(out)\n",
    "         \n",
    "        out = self.fc3(out)\n",
    "        out = self.elu3(out)\n",
    "\n",
    "        out = self.fc_time(out)\n",
    "\n",
    "        out = out.view(-1, self.step, self.feature_y)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5489376-749d-4118-82c5-335100f1cc45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, feature_x=5, feature_y=1, hidden=64, num_layers=3, time_window=96, step=16):\n",
    "        super(RNN, self).__init__()\n",
    "        self.feature_y = feature_y\n",
    "        self.step = step\n",
    "        self.hidden = hidden\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Calculate the flattened input dimension\n",
    "        flattened_input_dim = feature_x\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size=flattened_input_dim, \n",
    "                          hidden_size=hidden, \n",
    "                          num_layers=num_layers, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        # Mapping to the time steps\n",
    "        self.fc_time = nn.Linear(hidden, step * feature_y)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden).to(x.device)\n",
    "        \n",
    "  \n",
    "        out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc_time(out[:, -1, :])  # Taking the last time step's output\n",
    "        out = out.view(-1, self.step, self.feature_y)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b553cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, feature=5, feature_y=1, hidden=64, num_layers=5, time_window=96, step=16):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature, out_channels=hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden, out_channels=hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=hidden, out_channels=hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=hidden, out_channels=hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv1d(in_channels=hidden, out_channels=hidden, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        conv_out_size = hidden * time_window\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc1 = nn.Linear(conv_out_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, step * feature_y)\n",
    "     \n",
    "        self.step = step\n",
    "        self.feature_y = feature_y\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = x.view(-1, self.step, self.feature_y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8f6bf5-ae9f-4fe7-93ed-7fdacad4e747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dx_GRU(nn.Module):\n",
    "    def __init__(self, feature=5, feature_y=1, hidden=64,num_layers=3, time_window=96, step=16):\n",
    "        super(dx_GRU, self).__init__()\n",
    "        \n",
    "        # self.fc0 = nn.Linear(feature, 16)\n",
    "        self.gru = nn.GRU(feature, hidden, num_layers, batch_first=True, dropout=0.3)\n",
    "        # self.gru = nn.GRU(feature, hidden, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(time_window, step)\n",
    "        self.fc2 = nn.Linear(hidden, feature_y)\n",
    "        #self.fc2 = nn.Linear(hidden*2, feature_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        \n",
    "        y, hidden  = self.gru(x)\n",
    "\n",
    "        y = self.fc1(y.permute(0, 2, 1))\n",
    "        y = self.fc2(y.permute(0, 2, 1))\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b65c7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, feature=5, feature_y=1, hidden=64,num_layers=3, time_window=96, step=16):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        # self.fc0 = nn.Linear(feature, 16)\n",
    "        #self.gru = nn.GRU(feature, hidden, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.gru = nn.GRU(feature, hidden, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(time_window, step)\n",
    "        #self.fc2 = nn.Linear(hidden, feature_y)\n",
    "        self.fc2 = nn.Linear(hidden*2, feature_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        \n",
    "        y, hidden  = self.gru(x)\n",
    "\n",
    "        y = self.fc1(y.permute(0, 2, 1))\n",
    "        y = self.fc2(y.permute(0, 2, 1))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5e1d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, feature=5, feature_y=1, hidden=64,num_layers=3, time_window=96, step=16,kernel_size=1,stride=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # self.fc0 = nn.Linear(feature, 16)\n",
    "        # self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=16, nhead=8), num_layers=3)\n",
    "        #64*96*5\n",
    "    \n",
    "        self.lstm = nn.LSTM(5, hidden, num_layers, batch_first=True, dropout=0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(time_window, step)\n",
    "        # self.fc2 = nn.Linear(hidden*2, feature_y)\n",
    "        self.fc2 = nn.Linear(hidden, feature_y)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        # x = self.transformer(x)\n",
    "        # y, (hidden, context) = self.lstm(x)\n",
    "        \n",
    "        y, hidden  = self.lstm(x)\n",
    "        \n",
    "        \n",
    "        # y = self.cnn(y.permute(0, 2, 1))\n",
    "        # y = self.fc1(y)\n",
    "        # y = self.fc2(y.permute(0, 2, 1))\n",
    "\n",
    "        y = self.fc1(y.permute(0, 2, 1))\n",
    "        y = self.fc2(y.permute(0, 2, 1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6927fb96-d60a-4e28-9c75-c1d7d07f1bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, feature=5, feature_y=1, hidden=64,num_layers=3, time_window=96, step=16,kernel_size=1,stride=1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        # self.fc0 = nn.Linear(feature, 16)\n",
    "        # self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=16, nhead=8), num_layers=3)\n",
    "        self.lstm = nn.LSTM(5, hidden, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(time_window, step)\n",
    "        # self.fc2 = nn.Linear(hidden*2, feature_y)\n",
    "        self.fc2 = nn.Linear(hidden*2, feature_y)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        # x = self.transformer(x)\n",
    "        # y, (hidden, context) = self.lstm(x)\n",
    "        \n",
    "        y, hidden  = self.lstm(x)\n",
    "        \n",
    "        \n",
    "        # y = self.cnn(y.permute(0, 2, 1))\n",
    "        # y = self.fc1(y)\n",
    "        # y = self.fc2(y.permute(0, 2, 1))\n",
    "\n",
    "        y = self.fc1(y.permute(0, 2, 1))\n",
    "        y = self.fc2(y.permute(0, 2, 1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65254e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=5, hidden_size=64, num_layers=3, seq_length=96, output_size=16, kernel_size=3, stride=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size, stride=stride, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        conv_out_size = (seq_length - kernel_size + 2 * 1) // stride + 1\n",
    "        pool_out_size = conv_out_size // 2\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size, output_size * 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = out.view(out.size(0), -1, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173de5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerLSTM(nn.Module):\n",
    "    def __init__(self, feature_x=5, feature_y=1, hidden=64, num_layers=3, time_window=96, step=16, kernel_size=1, stride=1):\n",
    "        super(TransformerLSTM, self).__init__()\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_x,\n",
    "            nhead=5, \n",
    "            dim_feedforward=2048, \n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=feature_x, hidden_size=hidden, num_layers=6, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(time_window, step)\n",
    "        self.fc2 = nn.Linear(hidden, feature_y)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transformer forward\n",
    "        transformer_out = self.transformer(x)\n",
    "        \n",
    "        # LSTM forward\n",
    "        lstm_out, _ = self.lstm(transformer_out)\n",
    "\n",
    "        #batch_size, seq_len, d_model = transformer_out.size()\n",
    "        #out = transformer_out.view(batch_size * seq_len, d_model)\n",
    "        y = self.fc1(lstm_out.permute(0, 2, 1))\n",
    "        y = self.fc2(y.permute(0, 2, 1))\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f17c44-fa6c-4636-9fb1-ef559d175ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_attention = InputAttention(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.zeros(x.size(0), self.lstm.hidden_size).to(device)\n",
    "        c = torch.zeros(x.size(0), self.lstm.hidden_size).to(device)\n",
    "\n",
    "        enc_outputs = []\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            input_weights = self.input_attention(x[:, -time_window+t:, :], h)\n",
    "            weighted_input = torch.mul(input_weights, x[:, -time_window+t:, :])\n",
    "            h, c = self.lstm(weighted_input.mean(dim=1), (h, c))\n",
    "            enc_outputs.append(h.unsqueeze(1))\n",
    "\n",
    "        enc_outputs = torch.cat(enc_outputs, dim=1)\n",
    "        return enc_outputs[:, -16:, :]\n",
    "\n",
    "class InputAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(InputAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(input_size, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        h = h.repeat(x.size(1), 1, 1).transpose(0, 1)\n",
    "        x = self.W1(x)\n",
    "        h = self.W2(h)\n",
    "        e = torch.tanh(x + h)\n",
    "        attention_weights = torch.softmax(self.v(e), dim=1)\n",
    "        return attention_weights\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.U = nn.Linear(input_size, hidden_size)\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        h = h.repeat(x.size(1), 1, 1).transpose(0, 1)\n",
    "        x = self.U(x)\n",
    "        h = self.W(h)\n",
    "        e = torch.tanh(x + h)\n",
    "        attention_weights = torch.softmax(self.v(e), dim=1)\n",
    "        return attention_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.temporal_attention = TemporalAttention(hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, enc_outputs):\n",
    "        h = torch.zeros(enc_outputs.size(0), self.lstm.hidden_size).to(enc_outputs.device)\n",
    "        c = torch.zeros(enc_outputs.size(0), self.lstm.hidden_size).to(enc_outputs.device)\n",
    "\n",
    "        dec_outputs = []\n",
    "        for t in range(enc_outputs.size(1)):\n",
    "            temporal_weights = self.temporal_attention(enc_outputs, h)\n",
    "            context_vector = torch.sum(temporal_weights * enc_outputs, dim=1)\n",
    "            h, c = self.lstm(context_vector, (h, c))\n",
    "            out = self.fc(h)\n",
    "            dec_outputs.append(out.unsqueeze(1))\n",
    "\n",
    "        dec_outputs = torch.cat(dec_outputs, dim=1)\n",
    "        return dec_outputs\n",
    "\n",
    "class DA_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DA_RNN, self).__init__()\n",
    "        self.encoder = Encoder(input_size,hidden_size)\n",
    "        self.decoder = Decoder(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print(x.shape)\n",
    "        enc_outputs = self.encoder(x)\n",
    "        #print(enc_outputs.shape)   \n",
    "        dec_output = self.decoder(enc_outputs)\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7892cce-2929-496a-8d0f-edc11b4b7bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "# Defines the overall TCN structure\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(output_size)\n",
    "        self.linear = nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.tcn(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8cb2d5-88c2-498a-8d6c-6b5f457b4212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrendSeasonalDecomposition(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.trend = nn.Linear(in_features, in_features)\n",
    "        self.seasonal = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend_part = self.trend(x)\n",
    "        seasonal_part = self.seasonal(x)\n",
    "        return trend_part, seasonal_part\n",
    "\n",
    "class EnhancedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size * heads)\n",
    "        self.keys = nn.Linear(embed_size, embed_size * heads)\n",
    "        self.queries = nn.Linear(embed_size, embed_size * heads)\n",
    "        self.fc_out = nn.Linear(heads * embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        B = queries.shape[0]\n",
    "        values = self.values(values).view(B, -1, self.heads, self.embed_size).transpose(1,2)\n",
    "        keys = self.keys(keys).view(B, -1, self.heads, self.embed_size).transpose(1,2)\n",
    "        queries = self.queries(queries).view(B, -1, self.heads, self.embed_size).transpose(1,2)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(B, -1, self.heads * self.embed_size)\n",
    "        \n",
    "        return self.fc_out(out)\n",
    "    \n",
    "    \n",
    "class AutoformerEncoder(nn.Module):\n",
    "    def __init__(self, feature_size, heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                TrendSeasonalDecomposition(feature_size),\n",
    "                EnhancedMultiHeadAttention(feature_size, heads),\n",
    "                nn.LayerNorm(feature_size),\n",
    "                nn.Dropout(dropout)\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layer_stack:\n",
    "            trend_layer, attention_layer, norm_layer, dropout_layer = layer\n",
    "            trend, seasonal = trend_layer(x)\n",
    "            x = attention_layer(seasonal, seasonal, seasonal, mask) + trend\n",
    "            x = norm_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbe5e8b-0e3d-416f-a9ad-b499f7d23bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoformerTCN(nn.Module):\n",
    "    def __init__(self, feature_size, num_channels, kernel_size, dropout, heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoformerEncoder(feature_size, heads, num_layers, dropout)\n",
    "        self.decoder = TemporalConvNet(feature_size, num_channels, kernel_size, dropout)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(16)\n",
    "        \n",
    "        self.final_layer = nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.decoder(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        x = self.final_layer(x.transpose(1, 2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a9dd0-b421-45fe-95c4-bc9269fa4ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
